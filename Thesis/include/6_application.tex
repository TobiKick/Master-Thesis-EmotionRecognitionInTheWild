
\chapter{Application}\label{chap:Application}
For PPI AG, the goal is to use the research conducted during this Master thesis as a scientific basis for a future application of this technology in one of their own business use cases. Therefore, this chapter will assess, whether emotion recognition is a viable assistance for consultants, as well as customers in the selected use case of video-call consultations.
\newline\newline
Firstly, a prototype was developed to prove that emotion recognition is a viable option for real-time predictions from a webcam stream. Secondly, a more meaningful interpretation of emotions was needed to provide consultants or customers with a more actionable metric. Interest was chosen to be the metric used to identify a customer's intention. Thus, an user experiment was conducted that compares the predicted interest based on emotions recognized from facial expressions with the stated level of interest by participants.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Prototype}
The first prototype was constructed as a local application that takes its video input from the PC's webcam. When the user presses the 'SPACE' key, the program extracts the current frame of the video stream, detects the human face, recognizes its emotion in terms of valence and arousal, and prints the results onto the screen. A snapshot of this is provided in Figure \ref{fig:PrototypeRealTime}. 

\begin{figure}[htbp]
  \begin{center}
  \includegraphics[angle=0, width=0.8\textwidth]{Figures/webcam_foto.PNG}
  \caption[Prototype application]{The photo shows a screenshot taken from the prototype application. The application allows to recognize emotions in terms of valence and arousal in real-time from a webcam stream.}
  \label{fig:PrototypeRealTime}
  \end{center}
\end{figure}

\noindent It was from special interest to PPI AG whether such an approach would be suitable for real-time usage, especially in regards to utilizing landmarks. 
\newline\newline
A small test of 10 probes indicated that it took on average 0.7 seconds to perform emotion recognition without any landmarks in production. In comparison to that, additionally detecting landmarks with the dlib shape predictor \citep{Kazemi:2014:ShapePredictor} and feeding them into the model did not result in any delay, as it also took on average 0.7 seconds. Such a low number, with less than 1 second, proves that this approach is definitely suitable for real-time use cases. 
\newline\newline
However, it needs to be pointed out that this could only be achieved with the dlib shape predictor, unlike an Active Appearance Model which would likely need more time to be constructed. Furthermore, it is to be expected that the average duration of 0.7 seconds further decreases when taking into account that the prediction be performed on a powerful server with GPU support instead of a laptop computer.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{User Experiment (Interest)}
The experiment provided further insights into the utilization of emotion recognition for the identification of human interest. Thus, the goal was to be able to draw conclusions about the suspected correlation between recognized emotions and human interest. Before diving into the design and outcomes of the experiment, an identification approach of interest had to be created based upon current literature.
\newline\newline
There herein presented approach for identifying interest was based on the ideas from two different scientific contributions:
\begin{itemize}
    \item \citet{Kamaruddin:2016:MeasuringCustomerSatisfaction} determined a level of interest/appreciation by measuring how positive or negative an emotion (= valence) experience was on average.
    \item \citet{Poirier:2016:AdsFacialExpression} measured the effectiveness of ads by analyzing facial expressions. They claimed that the emotional journey is the strongest predictor of ad appreciation which is why they focused on the curve/profile created by the predicted emotion values for valence.
\end{itemize}

% \citet{Poirier:2016:AdsFacialExpression} measured the effectiveness of ads by analyzing facial expressions. \citet{Poirier:2016:AdsFacialExpression} claimed that the emotional journey is the strongest predictor of ad appreciation. The researchers predicted the emotional values for valence and arousal, and focused at the curve/profile created by values for valence. The researchers' assumption seemed to be that a certain profile of the valence curve is the main determinant of whether an ad was successful. While this sounds promising, it also raises the question of whether these ad appreciation in commercial ads also converts into actual positive ratings, or even product sales

% Even though their actual interval duration is not known, the FaceReader \citep{Noldus:2020:Facereader} application, a commercial solution that was tested during this Master thesis, already offers a calculation for 'Interest' that was based on a interval length of 2 seconds. Moreover, \citet{Kamaruddin:2016:MeasuringCustomerSatisfaction} introduced a threshold for determining a neutral emotion category.

\noindent Inspired by these approaches, the prototype application imitates some of this behaviour and based its 'Interest' measurement solely on valence. For each frame extracted every second from the video stream, valence is predicted. Interest was calculated by averaging the predicted values of valence for the last 5 frames.
% If the value of valence is below the threshold of -0.2 it is counted as 'negative', if it is above +0.2 it is counted as 'positive' and if it is inside those two threshold values it is regarded as 'neutral'. Depending on ratio of the number of emotions that were classified as 'positive' vs 'negative', the 'Interest' is determined with a range from 0 to +1.
% \newline\newline
% \textbf{Example calculation:}

% \begin{table}[H]
% \begin{center}
% \begin{tabular}{@{}cccl@{}}
% \toprule
% Image & Valence & Classif. & \multicolumn{1}{c}{Interest calculation} \\ \midrule
% \#1 & -0.58 & negative &  \\
% \#2 & -0.24 & negative &  \\
% \#3 & 0.00 & neutral & neutral is not considered \\
% \#4 & 0.17 & neutral & neutral is not considered \\
% \#5 & 0.36 & positive &  \\
% \begin{tabular}[c]{@{}c@{}}...\\ (up to 20)\end{tabular} & ... & ... &  \\
% \multicolumn{1}{l}{} & \multicolumn{1}{l}{} & \begin{tabular}[c]{@{}c@{}}COUNT:\\ 2x negative\\ 1x positive\end{tabular} & \begin{tabular}[c]{@{}l@{}}= (\#pos * 100) / (\#pos + \#neg)\\ = (1 * 100) / (1 + 2) = 33.3\end{tabular} \\ \bottomrule
% \end{tabular}
% \caption{Example: Interest Calculation}
% \label{tab:InterestCalculation}
% \end{center}
% \end{table}

% In the Table \ref{tab:InterestCalculation} an example calculation for Interest is presented. It starts with taking the last 20 frames/images taken (in this case only 5 are presented for illustration reasons). The Valence of these frames is predicted and subsequently classified as negative, neutral or positive depending on the threshold of -0.2 and +0.2. Afterwards, the number of frames classified are counted and the Interest is calculated with the following formula: (\#positive * 100)/(\#positive + \#negative). Thus, the result in the presented example is 33.33.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Experiment Design}
The goal of this experiment is to compare the test person's perceived subjective interest with the interest predicted by the before mentioned approach. In order to make this experiment objective and comparable between different participants, it was chosen that instead of direct interaction, participants get to watch commercial ads. Through these ads it was expected to get the objectivel comparable results which are still somewhat similar to real-life video calls, for example during a product presentation.
\newline\newline
For the analysis of the participants interest, the webcam video stream was needed to be recorded exactly at the same time when the video was played. Additionally, a mechanism had to be devised to calculate a level of interest from the recognized emotions. For this, an experiment had to be implemented that could be given to participants, and that allows them to indicate their own current level of interest.
\newline\newline
For such an implementation there were multiple considerations about the setup type. The possible options are presented in the following table, where a 'X' indicates the non-fulfillment of an indispensable quality of an setup, while '/' indicates a constraint and ':)' indicates that the quality is fully fulfilled by the setup.

\begin{table}[htpb]
\caption[Application comparison for user experiment]{The table compares various options for the implementation of a user experiment application. The depicted ratings speak in favor of a web-based frontend application based on ReactJS.}
\begin{center}
\begin{tabular}{lccccc}
\hline
\textbf{} & Effort & \begin{tabular}[c]{@{}c@{}}Web-Cam\\ Video\end{tabular} & \begin{tabular}[c]{@{}c@{}}Continuous\\interest\\rating\end{tabular} & \begin{tabular}[c]{@{}c@{}}Easily\\ deployable\end{tabular} & \begin{tabular}[c]{@{}c@{}}Results\\ retrievable\end{tabular} \\ \hline
\textbf{\begin{tabular}[c]{@{}l@{}}Questionnaire\\ (e.g. Google Forms)\end{tabular}} & low & {\color[HTML]{FE0000} \textbf{X}} & {\color[HTML]{FE0000} \textbf{X}} & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{32CB00} \textbf{:)}} \\
\textbf{\begin{tabular}[c]{@{}l@{}}Advanced survey tool\\ (e.g. Phonic)\end{tabular}} & low & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{FE0000} \textbf{X}} & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{FFC702} \textbf{/}} \\
\textbf{\begin{tabular}[c]{@{}l@{}}Packaged Local Appl.\\ (e.g. Python)\end{tabular}} & high & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{FE0000} \textbf{X}} & {\color[HTML]{FFC702} \textbf{/}} \\
\textbf{\begin{tabular}[c]{@{}l@{}}Web-based Server Appl.\\ (e.g. PythonAnyw.)\end{tabular}} & high & {\color[HTML]{FE0000} \textbf{X}} & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{32CB00} \textbf{:)}} \\
\textbf{\begin{tabular}[c]{@{}l@{}}Web-based Client Appl.\\ (e.g. ReactJS)\end{tabular}} & high & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{32CB00} \textbf{:)}} & {\color[HTML]{FFC702} \textbf{/}} \\ \hline
\end{tabular}
\label{tab:appExperiment}
\end{center}
\end{table}

\vspace{1cm}

\noindent The setups mentioned in Table \ref{tab:appExperiment} were verified from top to bottom. First, the questionnaire and survey tools were investigated, because they, on the one hand, allowed for a quick implementation, but also did not allow room for the required customization to conduct this experiment. A locally-run, stand-alone application would have accommodated all of this. However, due to its nature and size, it would be difficult to provide the participants with the program needed for the experiment and to retrieve its results. The best choice identified in this work was to develop a web-based application.
\newline\newline
A web-based server application hosted by PythonEverywhere would have fulfilled all the requirements, however, hooking onto participants’ webcams did not prove to be possible. As a result, it was decided to develop a browser-based standalone web-application with ReactJS. It allowed for the needed customization and only posed a constraint in terms of results retrieval (the participants had to download the results and send them manually back).
\newline\newline
The user interface for the developed application is comprised of the presented video to be watched by the participant, as well as another smaller video that captures and records the webcam of the participant. There is also a slider for the indication of the current interest level of the participant, as well as a 'Start', 'Stop' and 'Download' button. The following figure shows the default screen when accessing the web page:

\begin{center}
\begin{figure}[htbp]
  \begin{center}
  \includegraphics[angle=0, width=0.9\textwidth]{Figures/UserExperiment.PNG}
  \caption[Implemented UI for the user experiment]{The figure shows the experiment's user interface. It allows users to watch commercials and to continuously indicate his/her current level of interest. At the same time the webcam stream from the user's webcam is recorded and provided as a download afterwards.}
  \label{fig:InterfaceUserExperiment}
  \end{center}
\end{figure}
\end{center}

\noindent There are three video clips from YouTube embedded into the web page. These video clips are successful commercial ads that are supposed to induce a moderate range of emotions in participants.
\newline\newline
For participants to watch those video clips and indicate their currently experienced level of interest, they had to follow the following instructions:

\begin{enumerate}[noitemsep]
    \item Open the website: experiment.tobiaskick.com
    \item Make sure your laptop/PC camera is not blocked and works
    \item In case of a pop-up: Allow the usage of your camera
    \item Make sure you can see yourself in the small rectangular box on the right-hand side
    \item Start the video with a click on 'Start Capture'. This will also activate the recording of your face.
    \item Watch the video and simultaneously indicate your current level of interest by setting the slider on the appropriate level (from 0 to +10)
    \item Adjust your interest continuously. You can pause and resume the video/recording.
    \item When video finished: Download results by clicking on 'Download results'
    \item In case of a pop-up: Allow the download of multiple files at the same time. It should download one .csv file and a .webm file
    \item Go on to the next video by clicking on the button 'Next video'
    \item Start the same process again for the next video by clicking the button 'Start Capture'
    \item Finally, send the results, consisting of a *.webm and *.csv file for each video clip, back to tkick93@gmail.com (totalling 6 files). Alternatively, drop the files in the following Google Driver folder (link was attached) and indicate your name.
\end{enumerate}

\noindent After the participant conducted all these tasks, a *.webm and *.csv file was received for each video. The *.webm files had to be converted to MP4 with an Online Converter (frame rate of 24 per second). After that the VLC media player was used to filter frames so that it returned a frame for each second of the video.
\newline\newline
In the next step, these frames were read into a Python program that predicted the values for valence and arousal for each frame and calculated a measure of interest based on the previously presented approach. The results were then compared to the stated interest in the *.csv file.


\subsection{Experiment Outcomes}
The goal of the experiment was to compare the indicated level of interest of all participants with a calculated value of interest which was based on predictions from the proposed model architecture in this thesis.
\newline\newline
Due to different video conditions between participant's web camera, as well as the discrepancy between discretely stated interest by participants and continuously predicted interest, the model is not expected to be able to match the subjectively stated level of interest exactly. However, the underlying hypothesis is that there exists a trend between subjectively stated interest and continuously predicted interest, even though the magnitude might be different.
\newline\newline
This hypothesis was tested by calculating the Pearson Correlation between the interest stated by participants and the interest predicted with emotion recognition. An average was calculated over the correlation measure of all participants. 
\newline\newline
Generally speaking, a Pearson Correlation between 0.3 and 0.5 is regarded as a low correlation, between 0.5 and 0.7 as moderate correlation and above 0.7 as high correlation. In order to make the claim that emotional valence can act as an indicator for human interest, the goal was to achieve at least a correlation of 0.3.
\newline\newline
The metric for the experiment is the average Pearson correlation. The gathered data includes three presented video clips for each of the 10 participants (participants with invalid data were not considered here). For the prediction a neural network pre-trained on the AffWild dataset was chosen. This allowed to evaluate this experiment despite ongoing changes to the proposed model architecture during this thesis. The prediction of interest through the selected model achieved on average a Pearson correlation of 0.35. This number indicates a moderate correlation, from which can be concluded that the results were not just achieved by pure chance. As a result, this experiment could prove the existence of a meaningful correlation between emotional valence and the actual interest indicated by participants.
\newline\newline
Next to the actual result, the experiment gave way to further insights:\newline
\begin{itemize}
    \item \textbf{Emotional valence:} The emotional valence predicted by the neural network is not varying a lot (e.g. values bigger than +5 or smaller than -5 are very rare). This might indicate that the expression of emotions is really low during this experiment where participants were watching commercial ads. This is insight that should be kept in mind, as database usually contain material with high emotional content, while real-life data might generally be lower in emotional content.
    \item \textbf{Processing time for interest identification:} A sample of 151 frames was taken to conduct a probe on the processing time for interest identification. This probe revealed that the preparation of these frames, including the face detection, cropping and sequencing, took in total 17.21 seconds - 0.11 seconds per frame. The actual prediction of interest for these frames, that were fed into the model as one sequence, took in total 13.16 seconds - 0.09 seconds per frame (The analysis was conducted on a fairly standard laptop with a Intel Core i5-5200U CPU processor).
\end{itemize}

% \textbf{Viability in real-time video calls}
% especially focus on the time aspect for detecting the bounding box + landmark detection/shape model construction.

% Comparison dlib's shape model vs. Active Appearance Model
% \begin{quote}
%     The dlib 'Face Landmark Detection' algorithm is blazing fast, in fact it takes about 1–3ms (on desktop platform) to detect (align) a set of 68 landmarks on a given face.
% \end{quote} 
% https://medium.com/datadriveninvestor/training-alternative-dlib-shape-predictor-models-using-python-d1d8f8bd9f5c

% @InProceedings{Kazemi_2014_CVPR,
% author = {Kazemi, Vahid and Sullivan, Josephine},
% title = {One Millisecond Face Alignment with an Ensemble of Regression Trees},
% booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
% month = {June},
% year = {2014}
% }
