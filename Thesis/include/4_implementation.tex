\chapter{Implementation}
\section{Experimental Setup}
In order to conduct the training process described in the previous chapter, it was necessary to have a computer with a powerful GPU in order to speed up training. For this Master thesis, the experiments/training were conducted on a Nvidia Titan X (Pascal) GPU with 12 GB of memory. 
\newline\newline
For the ease of reproducibility of this work, frameworks and major libraries are hightlighted as follows: For machine learning, Keras 2.2.5 with TensorFlow 1.14.0 as a backend. In addition to that, Keras-VggFace 0.6, MTCNN 0.1.0 and OpenCV 4.1 were used. The entire code was implemented in a Python 3.6 environment.


\section{Dataset}
The selected Acted Facial Expressions in the wild - Valence Arousal (AFEW-VA) dataset introduced by \citet{Kossaifi:2017:AFEW-VADatabase} is based upon the Acted Facial Expressions in the wild database (AFEW) introduced in \citeyear{Dhall:2012:AFEW} by \citet{Dhall:2012:AFEW}. The AFEW dataset is composed of video clips that try to depict a real-world environment. It captures facial expressions, natural head pose movements, occlusions, subjects' races, gender, diverse ages, and multiple subjects in a scene. The authors labeled the video clips with one of six basic expressions: anger, disgust, fear, happiness, sadness, surprise, or neutral. The AFEW-VA dataset \citep{Kossaifi:2017:AFEW-VADatabase} uses the same underlying real-world video data, but did not annotate its video clips with one of six basic expressions. Instead it used the two dimensional affective space with valence and arousal. 
\newline\newline
Furthermore, as the name of the dataset already suggests, the dataset is compopsed of data that was collected under in-the-wild conditions. In-the-wild refers to real-life conditions in video clips, which make it significantly more challenging to recognize emotions in comparison to a dataset that has been captured in a controlled environment. \citet{Salah:2018:VideoBasedER} explained that these difficulties can be caused by, for example, uncontrolled illumination or uncontrolled video quality due to a different recording medium, like webcams by individuals vs. professional cameras. Such in-the-wild data is usually acquired from talk shows, movies or other natural interactions. 
\newline\newline
Since the research conducted during this Master's thesis is intended to serve as a basis for a further real-life application in video-call scenarios, it was clear that the dataset needed to be as close to in-the-wild conditions as possible. Furthermore, it was decided that it was more important to capture as much of an emotion's information as possible rather than placing value on the interpretation of emotions. As a result, the AFEW-VA dataset got chosen because of its in-the-wild conditions, its 2D Affective Space model for emotion representation and its backing by the scientific community when it comes to providing comparable results.
\newline\newline
The AFEW-VA dataset is made up of 600 video clips, each consisting of multiple frames that make up the video clip. Further statistics about the video-clips, frames and subjects of the dataset are presented in Table \ref{tab:DatasetStatistics}. Furthermore, each frame is then annotated in terms of valence and arousal. Both values have an annotation level ranging from -10 to +10 in full integer values. This results in a total of 21 levels.\citep{Kossaifi:2017:AFEW-VADatabase} 

\begin{table}[H]
\begin{center}
\begin{tabular}{@{}lc@{}}
\toprule
\textbf{AFEW-VA dataset statistics} &  \\ \midrule
total no. of video-clips & 600 \\
total no. of frames & 30051 \\
total no. of subjects & 240 \\
avg. no. of videos per subject & 2.5 \\
min. no. of frames per video & 10 \\
max. no. of frames per video & 145 \\
avg. no. of frames per video & 50.1 \\
median no. of frames per video & 45.5 \\ \bottomrule
\end{tabular}
\caption[AFEW-VA dataset statistics]{Gathered statistics about the no. of video-clips, frames and subjects in the AFEW-VA database.}
\label{tab:DatasetStatistics}
\end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Training \& Regularization}
\subsection{Backbone Network}

\begin{figure}[H]
  \begin{center}
  \includegraphics[angle=0, width=1.0\textwidth]{Figures/resnet50.png}
  \citep{Dwivedi:2019:ResNetInKeras}
  \caption[ResNet50 architecture overview]{The original ResNet50 \citep{He:2015:DeepResidualLearningForImageRecognition} architecture is a very deep-layered architecture consisting of multiple layers inside each of the five stages.}
  \label{fig:ResNet50Architecture}
  \end{center}
\end{figure}

The chosen ResNet50 model \citep{He:2015:DeepResidualLearningForImageRecognition} consists of 50 layers, combined in 5 stages, and includes over 23 million trainable parameters. It was one of the very first deep-layered neural networks that introduced a solution to the notorious vanishing gradient problem by its 'skip connection' concept.
\newline\newline
'Skip connection' is done by adding a shortcut from the input to the output of a 'CONV' or 'ID' block, allowing the gradient to flow through. This behaviour, as illustrated in Figure \ref{fig:ResNet50ConvBlock}, makes sure that the subsequent block performs at least as well as the previous. An overview of the composition of all blocks in ResNet50 is presented in Figure \ref{fig:ResNet50Architecture}. 

\begin{figure}[H]
  \begin{center}
  \includegraphics[angle=0, width=0.9\textwidth]{Figures/ResNet50_ConvBlock.png}
  \citep{Dwivedi:2019:ResNetInKeras}
  \caption[ResNet50 skip connection]{In a Convolutional block of ResNet50 \citep{He:2015:DeepResidualLearningForImageRecognition}, additionally to adding the result, a convolution is performed on the original input.}
  \label{fig:ResNet50ConvBlock}
  \end{center}
\end{figure}


% ResNet50 is defined by a very deep-layered neural network which traditionally posed a difficult challenge to researchers, as the training process got worse the deeper the neural network due to the notorious vanishing gradient problem. The vanishing gradient leads to a rapid saturation of the model's weights which results in a degradation of the overall model performance. To fight this problem, ResNet introduced a novel concept, named skip connection, where the results of a block are added together with the original input before applying an activation function. \citep{Dwivedi:2019:ResNetInKeras}
% \newline\newline
% This concept is also illustrated in the following Figure \ref{fig:ResNet50IdentityBlock} for the identity block:

%\begin{figure}[H]
%  \begin{center}
%  \includegraphics[angle=0, width=0.9\textwidth]{Figures/ResNet50_IdentityBlock.png}
%   \caption{In an Identity block of ResNet50 \citep{Dwivedi:2019:ResNetInKeras}, a shortcut is added by adding the results to the original input.}
%   \label{fig:ResNet50IdentityBlock}
%   \end{center}
% \end{figure}

% However, this operation is only possible when the two inputs for the addition have the same shape. Due to the layout of the ResNet50 architecture, this is inherently the case for the Identity Block. For the Convolution Block a further operation is needed to transform the block's original input into the same shape as the outcome of the Convolutional layers. As can be seen in Figure \ref{fig:ResNet50ConvBlock}, this was done by applying a separate Convolutional plus Batch Normalization layer to the original input and choosing its hyperparameters in a way that the output will be in the same shape as the outcome of the Convolutional layers.

% \citet{Dwivedi:2019:ResNetInKeras} argued that this approach is helpful in mitigating the problem of vanishing gradients, as its skip connection concept provided an alternate shortcut path for gradients to flow through. This ensured that the subsequent block will perform at least as well as the previous.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[H]
%   \begin{center}
%   \includegraphics[angle=0, width=1.0\textwidth]{Figures/model_architecture.PNG}
%   \caption{The neural network architecture is constructed by loading the pre-trained VGGFace model and adding a custom classifier.}
%   \label{fig:NNArchitecture}
%   \end{center}
% \end{figure}

% Figure \ref{fig:NNArchitecture} describes the architecture of the neural network with the following elements:

% \begin{itemize}
%     \item Line \#2: the pre-trained VGGFace model is being loaded with 'resnet50' as its neural network architecture. With the parameter 'include\_top' set to False allows to exclude the classifier of the pre-trained network on face detection.
%     \item Line \#5 to line \#10: The VGGFace model is either set to trainable or non-trainable. For the first three epochs of fune-tuning, the model gets set to non-trainable. Afterwards it is set to trainable, and allows the model to update its weights.
%     \item Line \#12: The output of the VGGFace model is being accessed and in line \#13 reduced in dimensionality in order to fit the input requirements for the following layer.
%     \item Line \#15 to line \#18 contains the classifier consisting of a Dense layer (1024 units), together with two Dropout and a Batch Normalization layer in order to improve generalization capabilities.
%     \item Line \#20 and \#21: The output for each evaluation metric is defined individually. This makes it possible to neatly visualize the outcomes separately. The 'tanh' activation function resized the output to a floating-point number in the range of -1 to +1.
% \end{itemize}

% The choice of using only a single Dense layer in the classifier was based on a comparison of fine-tuning strategies by \citep{Pittaras:2017:FineTuningStrategiesComparison}. Through this comparison of fine-tuning strategies on pre-trained neural networks, they could show that
% \begin{quote}
%     increasing the depth of a pre-trained network with one more fully-connected layer and fine-tuning the rest of the layers on the target dataset can improve the network’s concept detection accuracy compared to other fine-tuning approaches. \citep[~p. 103]{Pittaras:2017:FineTuningStrategiesComparison}
% \end{quote}

% Moreover, \citet{Pittaras:2017:FineTuningStrategiesComparison} could obtained their best results with a fine-tuning approach that replaced the pre-trained classifier of a network with a single Dense layer containing a high number of neurons.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Training} \label{sec:Training&Regularization}
\subsubsection{Pre-trained network}
A pre-trained network is a saved neural network that has previously learned spatial hierarchy of features on a large and general dataset. As recommended by \citet{Chollet:2017:DeepLearningPython} using a pre-trained neural network is a highly effective approach when dealing with a small image dataset. With around 30.000 frames (= 600 video clips), the AFEW-VA dataset \citep{Kossaifi:2017:AFEW-VADatabase} can be considered as such. The utilized VGGFace network was pre-trained on the large-scale face recognition dataset VGGFace2 \citep{Cao:2018:VGGFace2}. This has the advantage, that face recognition poses similar challenges as found in emotion recognition, and by using the pre-trained VGGFace network, already learnt information can be reused for the emotion recognition challenge.

\subsubsection{Fine-tuning}
In order to be able to fine-tune a pre-trained network, it first had to be adapted to the current challenge. Therefore, in this work the classifier got replaced with a Fully-Connected plus two RNN layers. These RNN layers help to capture the tempo-spatial information between frames of a video-clip. The design decision for the classifier was based on previous experiments conducted by \citet{Kollias:2019:AffWild}.
% This decision was based on a comparison of fine-tuning strategies conducted by \citet{Pittaras:2017:FineTuningStrategiesComparison} who found out that they achieved the best results fine-tuning results when only adding a single Dense layer with a high number of neurons.
\newline\newline
Due to the random initialization of the custom added classifier, the first step when fine-tuning the model was to train the classifier for a few epochs. At the same time, all other layers from the pre-trained model were not allowed to train and adapt to the new challenge (=frozen). This prevents the error stemming from the random initialization to propagate through the whole network.\citep{Chollet:2017:DeepLearningPython}
\newline\newline
In the second fine-tuning step, the pre-trained neural network was allowed to train and adapt to the new task in accordance with the selected Multi-Phase Fine-Tuning (FT) \citep{Sarhan:2020:MultiPhaseFineTuning} strategy. In contrast to Single-Phase FT where network is trained by unfreezing a set amount of layers at once, in Multi-Phase FT the network is trained by successively unfreezing layers in phases. This is illustrated in Figure \ref{fig:MultiPhaseFT}. In each phase the network's training process is started again, but each time more layers are allowed to be trained and adapted to the new task. As a result, each phase can build on the progress made by the previous phase. According to \citet{Sarhan:2020:MultiPhaseFineTuning} Multi-Phase FT achieves greater accuracy in fewer training epochs in comparison to Single-Phase Fine-Tuning.

\begin{figure}[H]
  \begin{center}
  \includegraphics[angle=0, width=0.8\textwidth]{Figures/MultiPhaseFT.PNG}
  \caption[Multi-Phase Fine-Tuning overview]{Multi-Phase Fine-Tuning (FT) leads to greater accuracy in fewer training epochs in comparison to Single-Phase FT.}
  \label{fig:MultiPhaseFT}
  \end{center}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Regularization}
A central role in developing a successful solution with machine learning is making sure that an algorithm will perform well not only on the training data, but also on previously unseen data. In many machine learning challenges it is common, that a well performing algorithm on the training data performs bad on previously-unseen data. This happens when the model is trying to memorize the training dataset, instead of learning its underlying patterns. This behaviour is called 'overfitting'. Strategies explicitly designed for decreasing overfitting, even at the expense of increasing the training error, are known as regularization. \citep{Goodfellow:2016:DeepLearning}
\newline\newline
An intuitive way to prevent overfitting is to use more training data, as this exposes the model to more data that it needs to find a useful representation for. However, in reality gathering more data is often not a viable option, which is why further regularization techniques are necessary to force the model to focus on the most prominent patterns. The easiest way, according to \citet{Chollet:2017:DeepLearningPython}, is to reduce the neural network's size in terms of trainable parameters. This would result in a reduction of the model's learning capacity.

\subsubsection{Feature removal}
For the here proposed machine learning model it was not possible to increase the amount of training data, as this would make an objective comparison with benchmark paper impossible. Thus, further regularization techniques were applied. The first choice was also to reduce the network's size. The pre-trained VGGFace network already provides a big stack of layers that cannot be removed without losing valuable information. Therefore, the convolution network for the mask, as well as the shared layers at the end of the network were kept at a minimal size.
% Therefore, the custom layers were reduced to a single Dense layer, which was proposed in a similar way by \citet{Pittaras:2017:FineTuningStrategiesComparison}, 

\subsubsection{Dropout}
Dropout is a very commonly used regularization technique that introduces noise during training by randomly setting a certain percentage of the layer's output values to zero. The Dropout applied in the proposed architecture was determined through extensive experimentation and was applied before and after the single Fully-Connected (=Dense) layer with a rate of 0.5 and 0.5 respectively. The following Figure shows an example with a dropout rate of 0.5.

\begin{figure}[H]
  \begin{center}
  \includegraphics[angle=0, width=0.7\textwidth]{Figures/dropout.PNG}
  \caption[Dropout regularization]{The applied dropout sets a defined percentage share of the parameters of an activation matrix to zero.\citep{Chollet:2017:DeepLearningPython}}.
  \label{fig:Dropout}
  \end{center}
\end{figure}

Applying such a dropout rate allows a network to purposefully forget some information, which makes the learning process harder, but merits a better generalization.

\subsubsection{Data Augmentation}


%%%%%%%%%%%%%
\section{Data Augmentation}

\begin{figure}[H]
  \centering
  \subfloat[V: 0.0, A: +0.5]{\includegraphics[width=0.5\textwidth]{Figures/001/001_6_00000.png}}
  \hfill
  \subfloat[V: 0.0, A: +0.5]{\includegraphics[width=0.5\textwidth]{Figures/001/001_6_00011.png}}
  \hfill
  \subfloat[V: +0.2, A: +0.3]{\includegraphics[width=0.5\textwidth]{Figures/002/002_6_00000.png}}
  \hfill
  \subfloat[V: +0.2, A: +0.4]{\includegraphics[width=0.5\textwidth]{Figures/002/002_6_00011.png}}
  \hfill
  \subfloat[V: -0.5, A: +0.3]{\includegraphics[width=0.5\textwidth]{Figures/576/576_6_00000.png}}
  \hfill
  \subfloat[V: -0.6, A: +0.3]{\includegraphics[width=0.5\textwidth]{Figures/576/576_6_00011.png}}
  \caption[Data Augmentation]{Visualization of augmented faces and their corresponding labels, expressed by valence (V) and arousal (A) as values ranging from -1 (V: very negative; A: very calming) and +1 (V: very positive; A: very exciting)}
  \label{fig:MethodologyDataAugmentation}
\end{figure}

Data Augmentation is another technique that is used to increase noise during training by randomly transforming existing training samples into slightly different looking images. In that way, the model will see more slightly different images and increases its ability to better generalize from training data. Even though data augmentation was applied during training to both, the face and it's corresponding mask, Figure \ref{fig:MethodologyDataAugmentation} only presents examples from augmented faces.
\newline\newline
The Data Augmentation applied in the here proposed solution augmented the images randomly with the following parameters:

\begin{itemize}
    \item rotation range from 0 to 15 degrees
    \item width shift range from 0 to 15 percent of the total width
    \item height shift range from 0 to 15 percent of the total height
    \item horizontal flip
    \item brightness shift range from 80 to 120 percent
\end{itemize}

\subsubsection{Hyper-parameter optimization} \label{sec:HyperParameterOptimization}
Optimization of important hyper-parameters also greatly reduces the effects of overfitting. Two of the most impactful parameters are the learning rate and the batch size.
\newline\newline
An analysis conducted by \citet{Yuanzhi:2019:RegularizationInitialLargeLearningRate} on Initial Learning Rates confirmed that an initially large learning rate can have a regularization effect on the training process. Even though a small initial learning rate might allow for better performance initially, it will not be able to generalize as well as initially large learning rates. The training performed in this Master's thesis made use of an initial learning rate of 0.0001 for the first 3 epochs. Afterwards the training was also started off with a rate of 0.0001, but continuously decreased with each epoch by a factor of 0.95 through a learning rate scheduler.
\newline\newline
Along with the effects of the initial learning rate on improving generalization and reducing the effects of overfitting, \citet{Keskar:2016:LargeBatchTrainingGeneralization} posited that choosing small-batch methods consistently generalize better than large batch methods. Due to memory constraints it was not possible to experiment with large batch sizes, which is why for the here proposed approach a batch size of 2 was chosen. This translates into each batch consisting of two times the sequence length of 45.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Evaluation \& Metrics}
\subsection{Evaluation} \label{sec:TrainValTestSplit}
Since the AFEW-VA dataset has no official validation and testing set, the AFEW-VA paper proposed by \citet{Kossaifi:2017:AFEW-VADatabase} and the simultaneous VA prediction paper proposed by \citet{Handrich:2020:SimultaneousPredVA} evaluate their models through splitting the AFEW-VA dataset into five disjoint, subject-independent folds. Subsequently, they performed a 5-fold-cross-validation for the prediction of valence and arousal values. 
\newline\newline
Due to time, complexity and resource constraints, it was chosen that this Master's thesis would not use the 5-fold-cross-validation evaluation approach. Instead, it makes use of the more prevalent method for machine learning model evaluation, namely the split of the dataset into a training, validation and testing subset. The training and validation subset were used during training to determine the optimal hyper-parameter settings. As these settings influence the performance on the validation subset, a third, independent subset was needed: the testing subset. This subset was used for a final performance evaluation after the optimal hyper-parameter were found.
\newline\newline
The following Figure \ref{fig:TrainValTestSplit} illustrates the chosen evaluation approach:

\begin{figure}[H]
  \begin{center}
  \includegraphics[angle=0, width=0.7\textwidth]{Figures/TrainValTestSplit.png}
  \caption[Dataset training-validation-test split]{The dataset was split into a training, validation and testing subset. While training and validation were used for the determination of optimal hyper-parameter settings, the testing subset was set aside for a final evaluation of the best performing model.}
  \label{fig:TrainValTestSplit}
  \end{center}
\end{figure}

% \begin{figure}[H]
%   \begin{center}
%   \includegraphics[angle=0, width=0.7\textwidth]{Figures/TrainTestSplit.png}
%   \caption{For the determination of optimal hyper-parameter settings, the dataset was split into a training, validation and testing subset.}
%   \label{fig:TrainTestSplit}
%   \end{center}
% \end{figure}

% As soon as optimal hyper-parameter settings were found during the training of the model, a 5-fold-cross-validation was conducted in order to make results comparable with the benchmark. For this 5-fold-cross-validation, previously determined optimal hyper-parameter settings were utilized for training and testing the model five times. As illustrated in Figure \ref{fig:CrossValidationSplit}, each time the model is trained during cross-validation, a different combination of folds was selected as training and testing subset.\newline

% \begin{figure}[H]
%   \begin{center}
%   \includegraphics[angle=0, width=0.7\textwidth]{Figures/CrossValSplit.png}
%   \caption{The dataset is split into five equal and subject-independent folds. Cross-Validation is performed by alternating the folds for training, validation and testing.}
%   \label{fig:CrossValidationSplit}
%   \end{center}
% \end{figure}

\subsection{Metrics} \label{sec:Metrics}
Considering the nature of the problem at hand, predicting the values for valence and arousal is better solved through regression, hence a metric such as accuracy, albeit common, will not provide useful information to reflect the performance of the system. Therefore, we stick to the measures used by \citet{Kossaifi:2017:AFEW-VADatabase}, namely root-mean-square error (RMSE) and Pearson product-moment correlation coefficient (CORR). 
\newline\newline
RMSE (eq. \ref{eq:RMSE}) is useful in giving the observer a notion of how close the predicted values are to the actual values, while CORR (eq. \ref{eq:CORR}) tells how strong the relationship between the prediction and the actual label is \citep{2020:RMSE} \citep{2020:PearsonCorrelation}.
  
\begin{equation} \label{eq:RMSE}
RMSE = \sqrt{(\frac{1}{n})\sum_{i=1}^{n}(y_{i} - x_{i})^{2}}
\end{equation}
\newline\newline
\begin{equation} \label{eq:CORR}
CORR = \frac{{}\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})}
{\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}}
\end{equation}

where
\begin{conditions*}
 x_i  &  the actual ground truth value\\
 y_i  &  the predicted output value \\
 \overline{x}  &  the mean of all ground truth values \\
 \overline{y}  &  the mean of all predicted output values
\end{conditions*}

The RMSE is additionally used as the loss function for the optimization during training.

\subsection{Observation}
% Early Stopping & Model Checkpoint?
In order to find optimal hyper-parameter settings during training, the aforementioned metrics were continuously observed through an early stopping callback, as well as a model checkpoint callback. These callbacks made sure that the last best model, in terms of validation loss, was automatically backed up and that training was stopped as soon as the model successively failed to improve. The callbacks for this work are clearly defined in Figure \ref{fig:EarlyStopping} and \ref{fig:ModelCheckpoint}.

\begin{figure}[H]
  \begin{center}
  \includegraphics[angle=0, width=0.8\textwidth]{Figures/EarlyStopping.PNG}
  \caption[EarlyStoping callback]{The early stopping callback stops the training process as soon as there has not been any learning progress after a set period of epochs.}
  \label{fig:EarlyStopping}
  \end{center}
\end{figure}

\begin{figure}[H]
  \begin{center}
  \includegraphics[angle=0, width=0.8\textwidth]{Figures/ModelCheckpoint.PNG}
  \caption[Model checkpoint callback]{The model checkpoint callback tracks the best performing epoch according to a defined metric and automatically saves the weights of the best model.}
  \label{fig:ModelCheckpoint}
  \end{center}
\end{figure}

In summary, these callbacks reduce overfitting by stopping the learning process when no improvement on validation data was detected. Additionally, they allow to objectively compare different hyperparameter-settings of a model's architecture with one another.
